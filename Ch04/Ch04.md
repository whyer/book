
# RavenDB concepts

We have a running instance of RavenDB, and we have already seen how we can put and get information out of our database. But we are still only just scratching the surface of what we need to know to make effective use of RavenDB. In this chapter, we'll go over the major concepts inside RavenDB. 

The first step along the way is to understand what documents _are_.

## Entities, Aggregate Roots and Documents

When using a relational database, you are used to using Entities (hydrated instances of rows from various tables) that makes up a single Aggregate Root. There is also the relatively minor concept of Value Objects, but those tend to be underused, because you _have_ to have a distinct identity for many things in your domain that don't really require it. The classic example is the order line. It has no independent existence, and it must be modified in concert with the whole order.
Yet, in a relational database, an order line must have its own primary key, and it is entirely feasible to change an order independently of the order it is associated with.

We'll dedicate Chapter 4, Document based modeling, for a full discussion on modeling behavior inside RavenDB, but here are the basics. In RavenDB, every document is an Aggregate Root. In fact, we generally don't even bother calling them Aggregate Roots, and just call them Entities. The distinction between an Aggregate and an Entity is only there because of the limitations of relational databases.

An Entity is-a document, and it isn't limited to simple structures such as a key/value map. You can model very complex structures inside a document. In the order and order line case, we'll not model the order and order lines independently. Instead, the order lines will be _embedded_ inside the order. Thus, whenever we want to load the order, we'll get all of the order lines with it. And modification to an order line (be it updating, removing or adding) is a modification to the order as a whole, as it should be.

The order line is now a Value Type, an object that only has meaning within its parent object, not independently. This has a lot of interesting implications. You don't have to worry about Coarse Grain Locking^[You might notice a lot of terms from the Domain Driven Design book used here, that is quite intentional. When we created RavenDB, we intentionally made sure that DDD applications would be a natural use case for RavenDB.] or partial entity updates. Rules like external references should only be to aggregates are automatically enforced, simply because documents _are_ aggregates, and they are the only thing you can reference.

Documents are independent and coherent. What does those mean? When designing the document structure, you should strive toward creating a document that can be understood in isolation. You should be able to perform operations on a document by loading that single document and operating on it alone. It is rare in RavenDB to need to reference additional documents during write operations. That is enough modeling for now, we'll continue talking about that in the next chapter. Now we are going to go beyond the single document scope, and look at what a collection of documents are.

## Collections

On the face of it, it is pretty easy to explain collections. See Figure 1 as a good example.

![The collections in the Northwind database](./Ch04/Figure01.png)

It is tempting to think about collections as a set of documents that has the same structure and are stored in the same location. That is not the case, however. Two documents in the same collection can be utterly different from one another in their internal structure. See Figure 2 for one such example.

![Two differently structured documents in the Users collection](./Ch04/Figure02.png)

Because RavenDB is schemaless, there is no issue with doing this, and the database will accept and work with such documents with ease. This allows RavenDB to handle dynamic and user generated content without any of the hard work that is usually associated with such datasets. It is pretty common to replace EAV^[Entity-Attribute-Value schemas, the common way to handle dynamic data in relational databases. Also notorious for being hard to use, very expensive to query and in general a trouble area you don't want to go into.] systems with RavenDB, because it make such systems very easy to build and use.

RavenDB stores all the documents in the same physical location, and the collection association is actually just a different metadata value. The `Raven-Entity-Name` metadata value controls which collection a particular document will belong to. Being a metadata value, it is something that is fully under you control. 

> **Collections & document identifiers**
> 
> It is common to have the collection name as part of the document id. So a document in the Products collection will have the id of `products/`. That is just a convention, and you can have a document in the Products collection (because its metadata has the `Raven-Entity-Name` value set to 'Products') while it has the name 'bluebell/butterfly'.

RavenDB does use the collections information to optimize internal operations. Changing the collection once the document is created is _not_ supported. If you need to do that, you'll need delete the document and create it with the same id, but a different collection.

We've talked about the collection value in the metadata, but we haven't actually talked about what _is_ the metadata. Let talk meta.

## Metadata

The document data is composed of whatever it is that you're storing in the document. For the order document, that would be the shipping details, the order lines, who the customer is, the order priority, etc.
But you also need a place to store additional information, not related to the document itself, but _about_ the document. This is where the metadata comes into place.

The metadata is also a JSON format, just like the document data itself. However, there are some limitations. The property names follow the HTTP Headers convention of being Pascal-Cased. In other words, we separate words with a dash and the first letter of each word is capitalized, everything else is in lower case. This is enforced by RavenDB.

RavenDB uses the metadata to store several pieces of information about the document that it keeps track of:

* The collection name - stored in the `Raven-Entity-Name` metadata property.
* The last modified date - stored in the `Last-Modified` metadata property^[This is actually stored twice, once as `Last-Modified` and once as `Raven-Last-Modified`, the first is following the RFC 2616 format and is only accurate to the second. The second is accurate to the millisecond.].
* The client side type - stored in the `Raven-Clr-Type` metadata property.
* The etag - stored in the `@etag` metadata property, and discussed at length later in this chapter.

You can use the metadata to store your own values, for example, `Last-Modified-By` is a common metadata property that is added when you want to track who changed a document. From the client side, you can access the document metadata using the following code:

	Product product = session.Load<Product>("products/1");
	RavenJObject metadata =  session.Advanced.GetMetadataFor(product);
	metadata["Last-Modified-By"] = currentUser.Name;

It is important to note that there will be no extra call to the database to fetch the metadata. Whenever you load the document, the metadata is fetched as well. In fact, we usually need the metadata to materialized the document into an entity.

> **Changing a document collection**
>
> RavenDB does _not_ support changing collections. While it is _possible_ to change the metadata value for `Raven-Entity-Name`, doing so is going to cause issues.
> 
> We have a lot of optimizations internally to avoid extra work based on the collection name, and no support whatsoever for changing it. We've tried to add support for this, or even just flat out error when / if you try to change the collection name, but either option proved to be too expensive.
>
> When you save a document, RavenDB can just throw the data into the disk as fast as possible, needing to check for a previous collection name has proven to be very expensive (needing to do a read per write) and hurt our performance, especially in bulk insert mode.
>
> If you need to change a document's collection, the supported way to do that is to delete it and then save it again, with the same document id.

Once you have the metadata, you can modify it as you wish, as seen in the last line of code. The session tracks changes to both the document and its metadata, and changes to either one of those will cause the document to be updated on the server once `SaveChanges` has been called.

Modifying the metadata in this fashion is possible, but it is pretty rare to do so explicitly in your code. Instead, you'll usually use [listeners](#listeners) to do this sort of work.

## Document Identifiers

A document id in RavenDB is how we identify a single document from all the rest. They are the moral equivalent for the primary key in a relational system. Unlike a primary key, which is unique per table, all the documents in a database share the same key space^[Remember, collections are a virtual concept]. 

> **Identifiers terminology**
> 
> Document identifiers are also called document keys, or just ids or keys. In the nomenclature of RavenDB, we use both keys and ids to refer to a document id.

### Don't use Guids

Therefore, it follows that one of the chief requirements of the document ids is that they would be unique. This turns out to be a not so trivial problem to solve. A simple way to handle that is to use a Guid, such as this one:

	92260D13-A032-4BCC-9D18-10749898AE1C

It is entirely possible to use Guids as document identifiers in RavenDB. But it is also possible to drive to work on a unicycle. Possible doesn't mean advisable. Guids are used because they are easy to generate, but they suffer from weaknesses when it comes to their use as a unique identifier, they are relatively big compare to other methods and are nonsequential.

Those two means that it is easy to get a database into a situation where it has to do a _lot_ more work just to get data in when you are using a Guid. But that isn't their chief problem. The problem is that they are utterly opaque to humans. We often use identifiers for many purposes. Debugging and troubleshooting are not the least of those.

And having to look at 92260D13-A032-4BBC-9D18-10749898AE1C and see what we did with it along the way is not a really good way to spend your time. If you ever had to read a Guid over the phone, or keep track of multiple Guids in a log file, or just didn't realize that the Guid in this paragraph and the Guid higher up in the page aren't in fact the same Guid...

Guids aren't good for us. And by us, I mean humans.

### Human readable identifiers

A much better alternative is the default approach used by RavenDB, using the collection name as a prefix with a numeric id to distinguish different documents. You've already seen examples of this default approach. We have "products/1", "orders/15", etc.

This approach has several advantages. It tends to generate small and sequential keys, and most importantly, these type of keys are human readable and easily understood.

The question now is, how do we get this numeric suffix? 

### High/low algorithm

The problem with generating unique values is that you might not be the only one that want to generate them at this particular moment in time. So we have to ensure that we don't get duplicates.

One way to do that is to use a single source for id generation, which will be responsible for never handing out a duplicate value. RavenDB supports that option, and you can read about it in the next section, Identity.
However, such an approach requires going to the same source each and every time that we need to generate an identifier. 

The default approach used by RavenDB is quite different. We use a set of documents call the hilo documents. Here is a list of those documents in the Northwind database:

* Raven/Hilo/categories
* Raven/Hilo/companies
* Raven/Hilo/employees
* Raven/Hilo/orders
* Raven/Hilo/products
* Raven/Hilo/regions
* Raven/Hilo/shippers
* Raven/Hilo/suppliers

Those are pretty trivial documents, they all have just a single property, `Max`. That property's value is the maximum possible number that has been generated (or will be generated) for that collection.
When we need to generate a new identifier for a particular collection, we fetch that document and get the current max value. We then add to that max value and update the document.

We now have a range, between the old max value and the updated max value. Within this range, we are free to generate identifier with the assurance that no one else can generate such an identifier as well.

The benefit of this approach is that this also generates roughly sequential keys, even in the presence of multiple clients generating identifiers concurrently.

#### Self optimizing

The basis of the hilo algorithm is that a client that needs to generate 10 ids, it can take a range of 10 when it communicate with the server. From then on, it can generate those ids independently from the server until it runs out of ids.

Of course, you usually don't know upfront how many ids you'll want to generate, so you guess. By default, we use 32 as the default range, but the nice thing about the hilo approach is that it is the client that controls how much to take.

Remember how we said that RavenDB is self-optimizing? Here is one such case. When the client runs out of the reserved id range, it has to go back to the server to get a new reserved range. When it does so, it checks to see how much time has passed since the last time it had to go to the server. If the time is too short, that is an indication that we are burning through a lot of ids. In order to reduce the number of remote calls, the client will then request a range twice as big as before.

In other words, we start by requesting a range of 32. We consume that quickly, and we request a range of 64, and so on. Very quickly, we find the balance where we don't have to go to the server too often to get new ids. 

The actual mechanics are a bit more complex, because we scale up higher than just by a power of two in practice. We also have the ability to reduce the size of the range we request if we aren't going through the range fast enough. 

The actual details of how it works is not part of the algorithm, those are internal implementation optimization detail. But it is important to understand the benefits that you get when using this.

#### Concurrency

So far, we talked about hilo as if there was just a single client talking to the server. The question is, what happens when there are two clients requesting a range in the same time.

> **Requesting a hilo range**
>
> While the terminology we use is "requesting a range", the RavenDB server isn't actually aware of the hilo protocol in any meaningful way. Requesting a range is a process that involved loading the document, updating the Max value and saving it back. 
> 
> Aside from those basic operations, all the behavior for hilo is implemented client side.

As you'll see toward the end of the chapter, RavenDB supports optimistic concurrency, and the hilo protocol takes advantage of this. When we save the document back after raising the Max value, we do so in a way that would throw a `ConcurrencyException` if the document has been changed in the meantime. 
If we get this error, we retry the entire process from the beginning, fetching the document again, recording the current Max value, then saving with the new Max.

This way, we are protected against multiple clients overwriting one another's changes and generating duplicate ids.

#### Distributed hilo

Using the hilo algorithm, we only have to go back to the database once we run out of ids in the range we reserved. But what happens when we cannot contact that database? We'll touch on distribution model later on, in Part 3, Scale Out, but I do want to expand on how this relates to hilo at this time.

Assuming that we have a RavenDB cluster made of 3 nodes. We will configure each node to have its own unique hilo prefix. This way, if the primary node is down, we can still reserve ranges, and we don't have to worry about reserving the same range as another client because of a network failover. 

We'll discuss such scenarios extensively in Part 3, for now, all you really care about is that you can use the hilo system in a cluster without worrying about a single point of failure.

#### Manual hilo

You don't need to do anything special to use the hilo algorithm. It is what the RavenDB Client API does by default. It generate ids that have the following format:

* orders/13823
* products/7371
* PackageTracking/3824822^[When the entity name is composed of a single word, we'll default to lower casing it, when it is composed of multiple words, we'll preserve the casing.]

But sometimes you want to just have the numeric id and work with that. Maybe you are working with internal ids (see Chapter 5, Modeling) or using semantic ids (see a bit later in this chapter) but for whatever reason, you want to be able to generate those hilo values yourself. 

You don't need to start implementing everything from scratch. You can just write the following code:

	HiLoKeyGenerator hiloKeyGenerator = new HiLoKeyGenerator("tags", 8);
	long id = hiloKeyGenerator.NextId(documentStore.DatabaseCommands);

The `hiloKeyGenerator` instance should be a singleton, do _not_ try to create a new hilo whenever you need an id. That would require us to reserve a new range every time.
The `HiLoKeyGenerator` constructor accepts the hilo name and the initial size of the range. The actual size of the range will change, according to your actual usage. 

Most of the time, when we call `NextId`, we will never have to go to the server, we can just increment the internal value and as long as we are in range, we are good. This is effectively what the RavenDB Client API does for you, but we usually default to a range of 32.

You can use the generated id to do quite a lot of good, and you benefit from not having to go to the server all the time.

#### The downside for hilo

The hilo algorithm is the default approach for generating ids in RavenDB for a reason. It is simple, efficient and scalable and it generates very nice identifiers. _Is_ there a downside?
Of course there is, if only because someone ate my free lunch.

The one downside for hilo is that it can generate nonconsecutive ids. What do I mean by that? Let us assume that we need to save several new orders. Using hilo, we generate the following ids for them: orders/65, orders/66, orders/67. Then we restart the application, and save some more orders.

Because hilo reserve a range, and there is no way to _unreserve_ part of that range, that means that this range has been lost. In this case, we reserved the range 65 - 96, but after generating just 3 such ids, we have been restarted. The range has been lost, and now we'll generate ids such as: orders/97, orders/98, etc. 

In practice, that isn't really a big downside. You might lost a few ids if the application restarted in production, but you'll likely not notice that. It is most often in development, where it is common to restart the application frequently, that people notice and wonder about this behavior.

But the actual reason that this isn't a big issue is that the ids that the RavenDB Client API generates aren't meaningful on their own. It doesn't actually matter if an order document's id is: orders/58 or orders/61. So skipping ids isn't something that we are generally need to concern ourselves with. When we do, we have the identity option.

### Identity

If you really need to have consecutive ids, you can use the identity option. Identity, just like in a relational database (sometimes called sequence) is a simple always incrementing value. Unlike the hilo option, you always have to go to the server to generate such a value. 

There are two ways to generate an identity value. The first is to do so implicitly, as shown in Listing 4. 

```{caption="Using implicit identity" .cs }  
using (var session = documentStore.OpenSession())
{
	var product = new Product 
	{
		Id = "products/", 
		Name = "What's my id?"
	};
	session.Store(product);
	Console.WriteLine(product.Id);
	// output: products/
	session.SaveChanges();
	Console.WriteLine(product.Id);
	// output: products/78
}
```

You can see that we actually define an id for the product. But a document id that ends with a slash (/) isn't allowed in RavenDB. We treat such an id as an indication that we need to generate an identity. That has an interesting implication. 

> **How identities are stored?**
> 
> Identities are stored^[This is a conceptual description, the actual storage is quite different] as a list of tuples containing the identity key and the last value. This data is persistent.
> In other words, if you delete the latest document, you won't get the same id back. 

> As a result of that, identities are actually created lazily, the first time we need them, and the first value generated is always one and there is no way to set a different step size for identities. 
> This raises the question, what happens if we create a document with the id "products/1" manually, then try to save a document with the id "products/"?
> 
>  RavenDB is smart enough to recognize this scenario, and it will generate a non colliding id in an efficient manner. In this case, we'll get the id "products/2"

We don't go to the server until we are actually calling `SaveChanges`. That means that we don't know what the actual document id is until after we already got the reply from the server. That isn't fun, but on the other hand, we can save multiple documents using identity without having to go to the server for each of them individually.

The other way to use identities is to do so explicitly. You can do that using the following code:

	long nextIdentity = documentStore.DatabaseCommands
		.NextIdentityFor("invoices");

This allows you to construct the full document id on the client side. But it does require two trips to the database, one to fetch the identity value and the second to actually save it. There is no way to get multiple identity values in a single request. 

You can set the identity next value using this command:

	long nextIdentity = documentStore.DatabaseCommands
		.SeedIdentityFor("invoices", 654);

> **Invoices, and other tax annoyances**
> 
> For the most part, unless you are using semantics ids (covered later in this chapter), you shouldn't care what your document id is. The one case you care is when you have an outside requirement to generate absolute consecutive ids. One such common case is when you need to generate invoices.
>
> Most tax authorities have rules about not missing invoice numbers, to make it just a tad easier to actual audit your system. But an invoice document's _identifier_ and the invoice _number_ are two very different things. 
> 
> It is entirely possible to have the document id of invoices/843 for invoice number 523.

#### The downsides for identity

There is no such thing as a free lunch, and identity also has its own set of drawbacks. Chief among them is that identities are actually not stored as a document. Instead, they are stored internally in a way that isn't quite so friendly.

That means that exporting and importing the database would _not_ also carry over the identities values. The identities values are also not replicated, so identity isn't suitable for use in a cluster. 

Finally, modifying an identity happens in a _separate transaction_ than the current transaction. In other words, if we try to save a document with the name "product/", and the transaction failed, the identity value is still incremented. So even though identity generate consecutive numbers, it might still skip ids if a transaction has been rollbacked. 

Except for very specific requirements, such as an actual legal obligation to generate consecutive numbers, I would strongly recommend not using identity. Note my wording here, a legal obligation doesn't arise because someone want consecutive ids because they are easier to grasp. Identity has a real cost associated with it.

### Semantic ids

Document ids in RavenDB do not actually have to follow the "products/43" format. A document id can be any string up to 1,024 Unicode characters^[That doesn't mean that is a _good_ idea to have a very long document id, long document ids require us to allocate more resources and do a lot more work internally. Ideal document ids are pretty short.], so you have a lot more options here.

One common scenario where you want to generate your own semantic ids is when you want to ensure that something is unique. Let us say that we wanted to make sure that we had unique user names. We can do that by naming the users documents with the actual user names:

* users/ayende
* users/john83
* users/zebrrra

This does several things at once, it allows us to ensure that there can never be a duplicate user name as well as allow us to load the document easily given just the user name. What if we don't have a username in our system, but just use the email[^unique-constraints]? We use the same approach:

[^unique-constraints]: What happens when you want to have _both_ username and email unique? That is where the RavenDB Unique Constraint Bundle comes to the rescue. This is discussed in Chapter 11, Bundles.

* users/ayende@ayende.com
* users/john83@example.org
* users/zebrrra@endofworld.left

Another reason to want semantic ids is to generate ids such as "customers/483/transactions/2014-08-06". As you can probably tell, this document is for Customer #483 and it contains all the transactions for Aug 6, 2014. Semantic ids are important in the context of modeling, and are discussed in Chapter 5, Modeling. 

Generating a semantic id is just a matter of setting the `Id` property of the document before calling `Store`. 

### Working with document ids

By now, you have a pretty good idea about document ids, and how they work. But that was almost entirely a discussion on how the client and the server generate ids. We haven't talked about actually _working_ with them.

RavenDB uses the `documentStore.Convenions.FindIdentityProperty` convention to figure out where the document is stored on your entities. By default, that is a property (or field) named "Id" (case sensitive). We have already talked about how to customize that in the previous chapter.

You probably realized it, but it is important to mention it explicitly. Identifiers in RavenDB are strings. That surprises people coming from relational background, where they are used to ids just being integers (or evil guids). 

> **Avoid `public int Id { get;set; }`**
> 
> If you define your `Id` property as an integer (or long, or Guid), everything will work. However, under the covers, the document id is still a string. What happens is that the RavenDB Client API will pretend take your "products/328" document id, strip off the first part (because the convention says that it is the collection name) and stick the numeric part in the id.
> 
> This matches what a lot of people is familiar with from working with relational database, and this feature is provided solely to make it easier to migrate to RavenDB. The problem with this approach is that the id is still a string. And there is a limit to how well we can pretend that it is an integer.
> 
> This usually comes up in the context of indexes or transformers, because those run on the server side, and we can't fake it out there. However, now you have a disconnection in your model, your client side code thinks it is an integer, and on the server side it is a string. Since indexes and transformers are actually defined on the client side, but executed server side, you can see how that would cause issues.

Given a document id, the most efficient way to get it to your hands is to `Load` it. Because it is a pretty common mistake to try to `Query` for a document id (which is several times more expensive), such an attempt is blocked and will throw^[You can set the convention option `AllowQueriesOnId` to allow that if you _really_ require this].

A document id cannot be changed once created, and attempt to associate two entity instances with the same document id, or attempt to change the document id on the entity once it was loaded or stored would result in an exception.

And that is quite enough about document identifiers. We'll now move to the other crucial piece of information that every document has, the ETag. 

## ETags

An etag in RavenDB is a 128 bit number that is associated with a document. 
Whenever a document is created or updated, an etag is assigned to that document. The etags are always incrementing, and they are heavily used inside RavenDB. Among their usages:

* Ensuring cache consistency
* Optimistic concurrency
* Indexing
* RavenDB replication
* Relational database replication
* Incremental exports

An etag is associated with the document metadata, so whenever you load the document, you also have the etag available for you. Retrieving the etag is easy, all you have to do is:

	Etag productEtag = session.Advanced.GetEtagFor(product);

On the client side, etags are used for optimistic concurrency control and for caching. We'll touch on optimistic concurrency in the next section, and caching is the section after that. I want to focus on how we are using etags on the server side.

> **The structure of an ETag**
>
> There is just one promise that we make about etags, and that promise is that they are always incrementing. Anything else is an implementation details. That said, it can be an _interesting_ implementation detail. Let us take a look at an etag: `01000000-0000-0001-0000-000000000EB6`.
> 
> This looks like a Guid, and indeed, this is a 128 bit number, which is using the Guid format convention because it is convenient. This is actually composed of the following parts:
>
> (@type) 01 
>
> (@restarts) 000000-0000-0001
>
> (@changes) 0000-000000000EB6
> 
> The first part is the etag type. 01 is etag for a document, this is the most common etag you'll run into. The second is the number of database restarts, this value is incremented by one every time the server restarts. In the case of our etag, it was generated on the first time the database was created.
>
> The last part is the number of changes that happened during the current database restart. All etags inside the same database restart period are consecutive. 
>
> There isn't really much cause for you to care about the actual content of an etag, but the question is raised often enough. Just remember that the details are _implementation details_, and might change in the future.

Because an etag is assigned to a document on every put^[RavenDB doesn't make a distinction between create or update.], and because etags are always incrementing, it is possible for RavenDB to iterate over documents in their _update_ order. So if I save products/1, products/2 and products/3 and then update products/1, when I'll iterate over them using the etag, I'll get results in the following order: products/2, products/3, products/1.

In fact, in the studio, when you are looking at all the documents, we default to sorting the documents by their last update. We do that by iterating over the documents in reverse update order, using the etags. But beyond the studio, iterating over the documents in update order turns to be _quite_ useful.

That is how RavenDB implements indexing and replication, among other features. The way it works is quite simple. We start from the empty etag (`00000000-0000-0000-0000-000000000000`) and ask the storage engine to give us all the documents that have an etag greater than that etag. RavenDB gives us a batch of documents to process. After we process those documents, we take the last document etag and remember it. Then we go back to the storage engine and ask us to give us all the etags after that etag. Rinse, repeat, and you have process through all the documents.

> **ETags in distributed system**
>
> An etag is only consistent within the node that defined it. RavenDB ensures that an etag is always incrementing within the node, but there is no coordination over etags in the cluster as a whole.

The reason that this works is that when a document is updated midway through this operation, we will see it again (because its etag was changed to a higher value). The actual behavior we have for indexing or replication is quite a bit more complex, but this is the basis for that. 

Now, let us see what other usages we have for etags...

## Optimistic Concurrency Control

What happens when we have two users that try to modify the same document at the same time? A document is a unit of change, and as such, trying to modify it concurrently is not allowed, so a `ConcurrencyException` will be thrown.
But actually managing to save the same document on the same instant to the server is pretty rare in most systems. Usually you are a lot more worried about the following scenario:

09:01 AM - John has loaded orders/3

09:02 AM - Martha has loaded orders/3

09:03 AM - John modify orders/3 and saves it

09:04 AM - Martha modify orders/3 and saves it

Note that at no point did we have any concurrency, each action happens at different times. Because they happened at different times, RavenDB can't tell that there is any issue, and Martha's changes will _overwrite_ John's changes. This behavior is call the Last Write Wins scenario. It is pretty useful when we don't have contention on our documents.

In many cases, we do want to detect and handle this scenario. You can ask RavenDB to do just that using the following code:

	session.Advanced.UseOptimisticConcurrency = true;

Once this is turned on, the session will make sure that when it sends the document to be saved in RavenDB, it will include the original etag it was fetched with. When the server actually gets the document to be saved, it will compare the specified etag to the current one, and it will throw a `ConcurrencyException` if they do not match. The server will rollback the current transaction and send the error to the client.

> **Why is Last Write Wins the default?**
> 
> Because this is what customers asked for, very loudly. Now, sometimes we get to tell a customer that as much as he wants a specific feature, that isn't something that is going to happen, because the ramifications of this feature will be destructive in the long run. Why didn't we do the same here? How is this Safe by Default?
> 
> Well, Safe by Default doesn't mean that we need to "protect" you by placing you in a padded room in a straightjacket. And there are several very common scenarios that make having Last Write Wins compelling. In particular, getting an entity through form binding and saving it to RavenDB directly, with very little code.
> Consider the following ASP.Net code:
> 
> ```
> public ActionResult Edit(Product product)
> {
>	DocumentSession.Store(product);
> }	
> ```
>
> Here, we are storing the object that we got directly from the ASP.Net model binding, without needing to first load the object. This is a very easy way^[Note that is has its pitfalls, such as lack of proper validation or business logic, but for some CRUD scenarios, this is perfect.] to handle data access.
> 
> But because we haven't loaded the document, we don't know what its etag, so we'll have to assume it is a new one, except that is already exists, so an error would be thrown, and users would be upset over disturbing this common scenario.
> 
> There are other, similar scenarios, so we made the call to default to Last Write Wins, and let you turn optimistic concurrency when you need it.

The `UseOptimisticConcurrency` setting affects all the operations in the session^[That means both `PUT` and `DELETE` operations that are being generated from `SaveChanges`. Optimistic concurrency is also available when patching documents, and will be discussed there as part of Chapter 6.], what happens if we want to have optimistic concurrency for only some documents?

You can force the RavenDB Client API to do an optimistic concurrency check on a single document using the following code:

	using (var session = documentStore.OpenSession())
	{
		var p1 = session.Load<Product>("products/1");
		var etag = session.Advanced.GetEtagFor(p1);
		session.Store(p1, etag);

		var p2 = session.Load<Product>("products/2");

		p1.QuantityPerUnit+=10;
		p2.Discontinued = true;

		session.SaveChanges();
	}

We aren't setting `UseOptimisticConcurrency` here, so we default to Last Write Wins, and indeed, this is what will happen if someone makes a change to the products/2 document. But because we explicitly called `Store(p1, etag)`, the session will save any updated to it _with_ optimistic concurrency^[Note that if the document was _not_ changed in the session, calling `SaveChanges` will not throw if the document was modified server side.]. There aren't many scenarios where you actually want to have Last Write Wins for a particular document and optimistic concurrency for another. This feature was created to enable end-to-end optimistic concurrency.

### End-to-end Optimistic concurrency

Just setting `UseOptimisticConcurrency` isn't going to be enough in most systems.  `UseOptimisticConcurrency` is relevant when all of the changes are done within the same session. However, in most situations, you are going to load a document, send it to the user, and then get a request in a few seconds or minutes that tell us to do something to that document.

	1> Load document products/2 and send it to John
	2> Load document products/2 and send it Martha
	3> Get update request for products/2 from John
	4> Load the document in a new session, update it per John's instruction and save it with `UseOptimisticConcurrency` = true;//todo: it is cut off in the release pdf
	5> Get update request for products/2 from Martha
	6> Load the document in a new session, update it per Martha's instruction and save it with `UseOptimisticConcurrency` = true;//todo: it is cut off in the release pdf
	7> John's updates are lost

The problem is in how we define what changed. In the scenario above, optimistic concurrency is within the scope of a single session. In Martha's update case, we loaded the document (which already had John's update) and then saved it with Martha's update. There is no problem as far as the database is concerned. The problem is that Martha never saw John's update, and as far as John & Martha are concerned, this is just another case of Last Write Wins.

In order to handle this scenario from an end to end perspective, we need to send to the client the etag of the document as well as the actual document itself. And when we request an update to the document, we'll need to pass along the _original etag value_. Listing 5 shows the server side code for end to end concurrency using ASP.Net MVC.

```{caption="End to end optimistic concurrency in RavenDB" .cs}
public ActionResult LoadProduct(int id)
{
	Product product = DocumentSession.Load<Product>(id);
	Etag etag = DocumentSession.Advanced.GetEtagFor(product);

	return Json(new
	{
		Document = product,
		Etag = etag.ToString()
	});
}

public ActionResult EditProduct(Product product, string etag)
{
	Etag originalEtag = Etag.Parse(etag);
	DocumentSession.Store(product, originalEtag);
	return Json(new {Saved = true});
}
```

We are sending the etag along with the document, and we get the original etag with the document. If the document has been changed between the `LoadProduct` and `EditProduct` request, we'll detect it and can show an error to the user.

By now we've seen that etags are important for index, replication and optimistic concurrency. There is another area where etags have a big role, caching. Let us see how that works in RavenDB.

## Caching

Caching is the very first tool that we use when we want to improve any system performance. In the context of a database, there are actually a lot of caching involved. We are precomputing things and storing them on disk for later^[This is the essence of how RavenDB's Map/Reduce work, for example], we cache indexes and documents in memory so we won't have to go to disk, and we also handle caching on the client side.

You don't actually care about the server side caching, that is an implementation detail. You certainly _benefit_ from that, but it has no impact on your day to day operations. The client stuff, however, it is very relevant. The first cache you'll encounter in RavenDB in the session cache:

	var p1 = DocumentSession.Query<Product>()
		.Where(p => p.Name = "Chef Anton's Gumbo Mix")
		.First(); // returns products/5
	var p2= DocumentSession.Load<Product>("products/5");

Here, we are only going to go to the database once. We first query the database for a product with the specified name, then we are loading a document by id. Since that document was already loaded into the session by the query, we could skip going to the server entirely. This is a very short lived cache, and it isn't actually there for performance. The main reason we have this behavior is so the session will implement the Identity Map pattern. Because the lifespan of the session is very short, you don't get a lot of utility of such a cache, but features such as include help make it a very important optimization technique.

Usually this is where you start using a cache provider to cache the results of queries, so you can avoid making a remote call to the database in the first place. However, RavenDB is a full service database, and we see no reason why you have to write caching code.

> **Hand rolled / ad hoc caching**
> 
> Caching should be a pretty simple process:
> 
> * Check the cache, if the value is there, return it. 
> * Otherwise, load it, put it in the cache and return it.
> 
> For something that is supposed to be so simple, it is actually _really_ complex when you go into the details^[The devil is there, and it _will_ poke you with a nasty pitchfork]. If the objects that you hold in the cache are mutable, then you can't actually return the same instance from the cache in multiple calls, you open yourself up for race conditions by having two threads fetch the same instance and modify it concurrently.
> 
> Cache misses are also pretty complex. What happens if you have two threads that have a cache miss at the same time on the same item? Do both of them fetch the value (increasing load on the database) or just one of them? 
> Cache invalidation is a topic that requires you to juggle multiple competing concerns (liveliness, performance, data staleness, and more)). 
> 
> Because of all those factors, caching code has the following properties:
> 
> * Boring
> * Multi-threaded
> * Repeatable
> * Performance critical
> * Quite tricky to get right
> 
> Combine all those properties together and you can probably see why writing caching code isn't something you want to do.

RavenDB exposes a REST interface to the outside world, and the nice thing about REST and HTTP is that a _lot_ of work has already been put into thinking about caching. RavenDB took full advantage of all of that work, and the RavenDB HTTP cache was born. 

### HTTP Caching

The [HTTP specification](http://tools.ietf.org/html/rfc2616#section-13) has a lot to say about [caching](http://tools.ietf.org/html/rfc2616#section-14.9). Most of which is dealing with how to ensure that the caches are correct enough. Inside RavenDB, we use the `ETag` and `If-None-Match` HTTP headers to create a very efficient caching system. Let us see how it works.

You want to load a document (by calling `DocumentSession.Load<Product>(1)`), which go through the session layer, to the database commands and finally to the REST layer. At that point, the call is converted into an HTTP request to a URI similar to this: `http://rvndb05:8080/databases/Northwind/docs?id=products/1`. 

Note that the generated URI is very important, that generates a cache key that we can check. In this case, this is the first HTTP request for that URI, so we have nothing in the cache for that. We make the request, and we get a reply back. Remember the etags we talked about in the previous section? They come into play here as well, because as part of the reply, we get the etag of the document.

Now we can put the HTTP response into the cache, under this URI as the key. The HTTP cache's scope is the _document store_, not the session. And since a `DocumentStore` instance is usually a singleton, that means that we have a single cache for all our operations. The next time we'll make a request to this URI, we'll have the result in the cache, so what will happen then?

At that point, we have a cache hit, and we _could_ just immediately return the result, but that wouldn't be safe. What happened if the document has changed in the database? We would return an outdated document. That would never work. What happens is that we _make_ a request to the server, even though we already have the previous response cached. But we send that request with an `If-None-Match` header set to the previous' request etag.

On the server side, we check if you have sent the `If-None-Match` header, and can check if the document in question has changed (by just comparing the etag from the client with the current etag). If the etag is the same, we can just return a 304 Not Modified response. This approach doesn't save us from having to do a network call, but it does save us from needing to transfer a lot of information over the wire, when we already have it cached in memory.

I've been talking about document loading, but the same process of using etags and the `If-None-Match` headers to check if the response has changed is used throughout RavenDB. When you are making a query, doing a multi load, using includes or fetching suggestions. The requests for all of those have an `ETag` header set^[This is easy in the case of a document, just use the document's own etag. But what happens if this is a single request for multiple documents, what would be the request's etag? The answer is that we hash all the documents etag together, and use the result as the request etag.], and they have optimized code paths, that can just answer whatever or not the request's etag has changed, specifically so the common case of "no, whatever you have in the cache is fine" will be very fast.

> **Customizing the cache**
> 
> It isn't common to need to modify the RavenDB HTTP cache, but we don't believe in utterly blocking our users, so there is actually quite a lot of control that you could assert over this process.
> 
> The cache is only active for GET requests, all other requests are ignored. And by default we cache all of them. A cache that doesn't have an eviction policy isn't a cache, it is a memory leak, and in this case, the RavenDB cache using the Least Recently Used algorithm, with a cap of 2,048 cached requests. You can change that by setting the `documentStore.MaxNumberOfCachedRequests` property.
> 
> A cache trade off memory for time, so the higher the number of cached requests, the more memory the cache will use, and the more your application could serve from the cache. One scenario where this is problematic is when you have many requests that return large number of big documents. The result is that the cache is filled with a lot of data, and that might cause issues.
> 
> You can fine tune what goes into the cache or not by using the `documentStore.Conventions.ShouldCacheRequest` event.

When you make a request with `If-None-Match` and the information has changed, we just process the request normally, and send the results to you along with the new etag. The end result is a system that is very fast, for the common case you only need to check with the server if something has changed, and you can save all the computation and bandwidth costs. At the same time, you don't have to worry about cache invalidation or displaying out of date results, because we confirm the accuracy of the results by checking with the server.

Having both performance and safety is great, that is why we have made this the default approach in RavenDB. But there is just one niggling issue still remaining. We have to go to the database to check if our information is still up to date. We can save the computational and bandwidth costs, but the _latency_ of going to the database is usually the most expensive part. That is why we have the next level of caching.

### Aggressive Caching

I was giving a talk once, and I asked the audience: what do you think aggressive caching means? One guy immediately said: "It is when the database beats back with a stick anyone that wants to change the data". And while that _would_ be a nice feature, it isn't quite what we have in mind.

Aggressive caching is an opt-in feature, and using it takes your caching to the next level. Let us see the code in Listing 6 first, then discuss it.

```{caption="Using aggressive caching" .cs}
using (documentStore.AggressivelyCache())
{
	for (int i = 0; i < 10; i++)
	{
		using (var session = documentStore.OpenSession())
		{
			var product = session.Load<Product>("products/1");
			Console.WriteLine(product.Name);
		}
		Console.ReadLine();
	}
}
```

Looking at Listing 6 code, how many requests would you expect? Without the `AggressivelyCache` call, we would expect there to be 10 requests. But with it? With `AggressivelyCache`, we are only going to make a single request to the server. Inside the aggressive cache scope, if we have a request in the cache, we don't even check with the server if it is up to date or not. That means that we can process pretty much whatever we want in memory, without ever having to make a single remote call.

You really can't get any faster than serving directly from your own local memory.

Of course, there is a downside, because we never check with the server, which might mean that someone will go and change the document on the server side, and we'll miss this update. Now you can probably see why we are calling this _aggressive_ caching. Except... that isn't how it really works.

Let us setup an experiment^[I suggest looking at the RavenDB console, which shows a list of all the requests made to the server. That can help you see when we are actually requesting data from the server and when we are serving directly from the cache.]. Put the code in Listing 6 in a console application, and hit enter a time or two. Then go and change the product name, and hit enter again. Because we are using aggressive caching, we aren't actually going to go to the server and check that we have the latest version, so we'll expect to see the same output. Instead, I got this:
	
	Chai
	Chai
	Latte

We have aggressive caching enabled, and we didn't check with the server, but we got the latest data anyway. How does that work?

Instead of having the RavenDB Client API check all the time if the cached information is up to date (polling), we are reversing the flow (pushing). The client asks the database to let it know if there has been any changes. As long as it didn't get a change notification from the database, the client can safety serve directly from the cache, with a high degree of confidence that the information it gives you is up to date. And when it does get a change notification, it merely needs to use the default caching route, where we go to the server to check if our results are actually up to date or not. 

> **Cache complexity**
> 
> It is easy to think that aggressive caching is going to use the details in the change notifications to selectively invalidate parts of the cache. However, that isn't the case^[I originally spelled this: "that isn't the cache", but decided that is was a bad pun]. The HTTP cache is pretty ignorant of the way RavenDB works, and even if we tried adding the knowledge to it, that would be very hard to handle.
>
> Consider for example the case of changing a specific product document. It is easy to see that the url for loading that document should be invalidated. But what about a url for documents by name? 
> Should it be invalidated? What about the url for an order that has an include for that product?
> 
> It is impossible to try to answer those questions without doing so much work that the benefits of cache would be nil, so we don't, instead we use a far simpler route, a change notification invalidate the entire cache.

Setting up aggressive caching also setup the change notifications subscription, and getting a change notification will cause the entire cache to be invalidated. That sounds scary, and far too aggressive^[I couldn't resist the pun this time]. Any change? The whole cache? 

That has got to pretty much kill this feature for real world purposes, right? But we aren't _clearing_ the cache, most of the data we already have in the cache is still going to be up to date. The way it works, each cache entry has a time, which mark when it was fetched from the database. And we track the time of the last change notification from the server. When the last change notification from the server is greater than the time the cache entry was fetched, we _have_ to go to the database to make sure that this is still consistent.

Because of this behavior, aggressive caching is almost perfect. You can usually serve the data directly from your own local cache, without having to make any remote calls, and at the same time, you are notified of any changes and can check with the server very quickly. Note that very quickly does not mean immediately. While the actual latency between a document being changed and the client being notified about this change is very short (in the order of milliseconds in the common case), that isn't zero^[It is also possible for a network problem to cause us to miss a change notification. In practice, that isn't an issue, because any change notification will force a check for everything, but it is something to be aware of, in terms of cache consistency.]. 

This means that it is possible for you to load the document from the cache even though 2 milliseconds ago it was changed. This violates one of the basic tenants of RavenDB, that access to the documents is fully ACID and immediately consistent. That is why you need to explicitly ask for this feature.
I wholeheartedly recommend taking advantage of this feature, but you need to consider what aspects of your code can accept potentially cached request, and what requires full consistency.

## Summary

Phew! This has been a long chapter. We covered a lot of the basic concepts in RavenDB from Entities & Aggregate Roots to collections and metadata. We then started to dive into deeper integration of your application with RavenDB.
On to identifiers, what not to do (Guids) and the various choices that you have with identifiers. Most importantly, in my opinion, identifiers should be human readable. Because they are a key part of how you work with your system. The RavenDB options we have, hilo, identity and semantic ids all follow the same principle. They are workable for humans first, and machines later.

After covering ids in great detail, we went on to etag, how they are composed and what we do with them. We look at the server side use of indexing and replication and at client side use of etags with optimistic concurrency (including end to end optimistic concurrency).

Finally, we looked at caching, and saw that on the client side, RavenDB has three different caching options. The session cache, which is primarily used for Identity Map. The HTTP cache, which uses the etags and `If-None-Match` header to check if the information has changed server side. Even better, we have Aggressive Caching, which can skip going to the server entirely, and can use change notifications to decide when to invalidate the cache.

On the next chapter, we'll continue to dive deeper into the client API. We'll talk about advanced features such as streaming results, bulk insert, subscribing to database changes and using partial document updates. After that, we'll move on to part two, indexing.
