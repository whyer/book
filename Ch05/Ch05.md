
# Advanced Client API Usage

With a good grounding in RavenDB Concepts, we can turn our mind to a more detailed exploration of the RavenDB Client API and what we can do with it.
We have already seen how we can use the client API for CRUD and querying (although we'll deal with that a lot more in [Part II](#part-ii)). Now we'll dive right in and see how we can get the most out of RavenDB. In particular, in this chapter we'll cover how we can get the most by doing the least.

## Lazy is the new fast

The [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing) were already covered in [chapter 1](#distributed-computing), but it is worth going over a couple of them anyway. This time, the relevant fallacy is: Latency is zero.

The reason that the Fallacies are so well known is that we keep tripping over them. Even experienced developers fall into the habit of thinking that their environment (everything is local) is the production environment, and end up making a lot of remote calls. But latency _isn't_ zero, and usually, just the cost of going to the database is much higher than actually executing whatever database operation we wanted. 

The RavenDB Client API deals with this in several ways. First, whenever possible, we batch calls together. When we call `Store` on a few entities, we don't go to the server. Instead, we wait for the call to `SaveChanges` which then saves all the changes in one remote call. Second, we have a budget in place on how many remote calls a single session can do. If your code exceeds this budget, an exception will be thrown. 

Because of this limit, we have a lot of ways in which we can reduce the number of times we have to go to the server. One such example is the [Includes feature](#include), which tells to RavenDB that we'll want the associated documents as well as the one we asked for. But what is probably the most interesting way to reduce the number of remote calls is to be lazy. Let us first look at Listing 5.1 and then we'll discuss what is going on there.

```{caption="Using Lazy Operations" .cs}
var lazyOrder = DocumentSession.Advanced.Lazily
	.Include<Order>(x => x.Company)
	.Load("orders/1");

var lazyProducts = DocumentSession.Query<Product>()
	.Where(x=>x.Category == "categories/2")
	.Lazily();

DocumentSession.Advanced
	.Eagerly.ExecuteAllPendingLazyOperations();

var order = lazyOrder.Value;

var products = lazyProducts.Value;

var company = DocumentSession.Load<Company>(order.Company);

// show order, company & products to the user
```

In Listing 5.1, instead of writing `DocumentSession.Include(...).Load(...)`, we used the `Advanced.Lazily` option, and instead of ending the query with a `ToList` we used the `Lazily` extension method. That much is obvious, but what does this _mean_?
When we use lazy operations, we aren't actually executing the operation. We merely register that we want that to happen. It is only when `Eagerly.ExecuteAllPendingLazyOperations` is called that those operation are executing, and they all happen in one round trip.

Consider the case of a waiter in a restaurant, when the waiter is taking the order from a group of people, it is possible for him to go to the kitchen and let the cook know about a new Fish & Chips^[This part of the book was written while I'm hungry, there might be additional food metaphores.] plate whenever a member in the group is making an order. But it is far more efficient to wait until every member in the group has ordered, and then going to the kitchen once.

> **Eagerly.ExecuteAllPendingLazyOperations vs. lazyOrder.Value**
> 
> In Listing 5.1, we have used the `Eagerly.ExecuteAllPendingLazyOperations` method to force all the lazy values to execute. But that is merely a convenience method. We could have done the same by just calling `lazyOrder.Value`. 
>
> At that point, the lazy object would initialize itself, realize that it hasn't been executed yet, and call `Eagerly.ExecuteAllPendingLazyOperations` itself. So the end result is very much the same, but we still prefer to call `Eagerly.ExecuteAllPendingLazyOperations` explicitly. 
> 
> It is easy to not notice that you are using the `lazyOrder.Value` property and trigger a query. I believe that it is much better when you have an explicit lazy evaluation boundary, rather than an implicit one. That is especially true if you are coming into the code several months or years after you wrote it.

That is exactly what lazy does. Except that it is actually even better. Lazy batch all the requests until `Eagerly.ExecuteAllPendingLazyOperations` is called, and then send them to the server. But on the server side, we unpack the request and then execute all of the requests _in parallel_. The idea is that we can reduce the overall latency by reducing the number of remote calls, and executing the requests in parallel means that we are done process everything so much faster.

Except for the deferred execution of the lazy operation, it behaves in exactly the same way. That holds true for options like `Include` as well, although, of course, the included document is only loaded into the session when the lazy operation has completed. Every read operation supported by RavenDB is also available to be executed as a lazy request.

On the server side, we buffer all the responses for the lazy request until all the inner requests are done, then we send them to the client. That means that if you want to get back a very large amount of data, you probably don't want to use lazy, or be prepare for increased memory usage on the server side. 

A better approach for dealing with very large requests is [streaming](#streaming-results), but before we get there, let us look at another way in which RavenDB is stopping bad things from happening.

## Unbounded Results Set Prevention

RavenDB was designed from the get go to be safe by default. One of the ways that this expresses itself is in the internal governors that it has. We have seen one such governor in the limit on the number of remote calls that a session can do. Another such governor is the limit on the number of results that will be returned from the database. Take a look at the following snippet:

	var orders = DocumentSession.Query<Order>()
		.ToList();

There are 880 orders in the Northwind database, how many results will this query return? As you can imagine, the answer is _not_ 880. Code like this snippet is _bad_, because it makes assumptions about the size of the data. What would happen if there were three million orders in the database? Would we really want to load and materialize them all? This problem is called Unbounded Result Set, and it is very common in production, because it sneaks up on you. You start out your system, and everything is fast and fine. Over time, more and more data is added, and you end up with a system that slows down. In most cases I've seen, just after reading all of the orders, we discarded 99.5% of them and just showed the user the last 15.

With RavenDB, such a thing is not possible. If you don't specify otherwise, all queries are assumed to have a `.Take(128)` on them. So the answer to my previous question is that the snippet above would result in 128 order documents being returned. Some people are quite upset with this design decision, their argument boils down to: "This code might kill my system, but I want it to behave like I expect it to". I'm not sure why they expect to kill their system, but they can do that without RavenDB's help^[You can still shoot yourself in the foot with RavenDB, but we like to think it would take some effort to do so.]. That way, hopefully it wouldn't be us that would get the 2 AM wakeup call and try to resolve what is going on.

Naturally, a developer's first response to hearing about the default `.Take(128)` clause is this:

	var orders = DocumentSession.Query<Order>()
		.Take(int.MaxValue) // fix RavenDB bug
		.ToList();

This is why we have an additional limit. You can specify a take clause up to 1024 in size. Any value greater than 1024 will be interpreted as 1024. Now, if you _really_ want, you can change that by specifying the `Raven/MaxPageSize` configuration, but we very strongly recommend against that. RavenDB is designed for OLTP scenarios, and there are really very few situations where you want to read a _lot_ of data to process a user's request.

For the situations where you actually do need all the data, the `Query` API isn't really a suitable interface for that. That is why we have result streaming in RavenDB.

## Streaming results

Large result sets should be rare in your applications. RavenDB is designed for OLTP applications, and there is very little cause for loading tens of thousands of documents and try showing them all to a user. They just have no way of processing so much information. It is much better to give them paging, so they can consume the data at a reasonable rate, and reduce the overall load on the entire system.

But there is one relatively common case where you do need to have access to the entire dataset: Excel. 

More properly, any time that you need to give the user access to a whole lot of records to be processed offline. Usually you output things in a format that Excel can understand, so the users can work with the data in a really nice tool. But reports in general are a very common scenario for this requirement.

So how can we do that? One way to handle that would be to just page, something like the abomination in Listing 5.2:

```{caption="The WRONG way to get all users" .cs}
public List<User> GetAllUsers()
{
	// This code is an example of how NOT
	// to do things, do NOT try to use it
	// ever. I mean it!
	List<User> allUsers = new List();
	int start = 0;
	while(true)
	{
		using(var session = DocumentStoreHolder.OpenSession())
		{
			   var current = session.Query<User>()
				   	.Take(1024)
				   	.Skip(start)
				   	.ToList();
			   if(current.Count == 0)
			        break;

			   start+= current.Count;
			   allUsers.AddRange(current);
		}
	}
	return allUsers;
}
```

I call the code an abomination because it has quite a few problems. To start with, this code will work just fine on small amount of data, but as the data size grows, it will do more and more work, and consume more and more resources. Let us assume that we have a moderate number of users, a 100,000 or so. 
The cost of the code in Listing 5.2 is: go to the server 98 times, do deeper paging on each request, hold the entire 100,000 users in memory. 

Note that the code there is also evil because it uses a different session per loop iteration, preventing RavenDB from detecting the fact that this code is going to hammer the database with requests. Another problem is when can we start processing this? What happens is the code buffers all the results, so we have to wait until the entire process is done to start handling this. All of those mean longer duration, higher memory usage and a lot of waste.

And what happens if someone is adding or deleting documents while this code is running? We don't make a single request, so it happens in multiple transactions, so there is also this issue. In short, never write code like this. RavenDB has built-in support for properly handling large number of results, and it is intentionally modelled to be efficient at that scale.
Say hello to streaming. Listing 5.3 is the same `GetAllUsers` method, now written properly.

```{caption="The proper way to get all users" .cs}
public IEnumerable<User> GetAllUsers()
{
	var allUsers = DocumentSession.Query<User>();
	IEnumerator<StreamResult<User>> stream =
		DocumentSession.Advanced.Stream(allUsers);
	while (stream.MoveNext())
	{
		yield return stream.Current.Document;
	}
}
```

Beside the fact that there is a lot less code here, let us take a look at what this code does. Instead of sending multiple queries to the server, we are making a single query. We also indicate that this is a streaming query, which means that the RavenDB Client API will have a very different behavior, and use a different endpoint for processing this query.

> **Paging and streaming**
>
> By default, a stream will fetch as many results as you need (up to 2.1 billion or so), but you can also apply all the normal paging rules to a stream as well. 
> Just as a `.Take(10 * 1000)` to the query before you pass it to the `Stream` method.

On the client side, the differences are:

* The use of enumerator to immediately expose the results as they stream in, instead of waiting for all of them to arrive before giving anything back to your code.
* The result of the `Stream` operation is _not_ tracked by the session. There can be a _lot_ of results, and tracking them would put a lot of memory pressure on your system. It is also very rare to call `SaveChanges` on a session that is taking part in streaming operation, so we don't lose anything.

The dedicated endpoint on the server side has different behavior as well. Obviously, it does not apply the `Raven/MaxPageSize` limit. But much more importantly, it will stream the results to you without doing any buffering. So the client can start processing the results before the server has finished sending them. Another benefit is consistency, as throughout the entire streaming operation, we are going to be running under a single transaction. 

What happens is that we operate on a database snapshot, so any addition, deletions or modifications are just not visible. As far as the streaming operation is concerned, the database is frozen at the time of the beginning of the streaming operation.

> **RavenDB Excel Integration **
> 
> I mentioned earlier that a very common use case for streaming is the need to expose the database data to users in Excel format. Because this is such a common scenario, RavenDB comes with a dedicated support for that. The output of the `Stream` operation is usually JSON, but we can ask RavenDB to output it in CSV format that can be readily consumed in Excel.
> You can go to the [Excel Integration documentation](http://ravendb.net/docs/http-api/excel-integration) page to see the walk through. 
>
> The nice thing about that is that you can even update the data from the database into Excel after the first import. 
>
> That said, please read the [Reporting Chapter](#reporting) for fuller discussion on how to handle reporting with RavenDB. In general just giving users access to the raw data in your database results in a mess down the road.

The `Stream` operation accept a query, or a document id prefix, or even just the latest etag that you have (which allow you to read all documents in update order). This is the usual way you're going to use to fetch large amount of data from RavenDB.

But what if I want to go the other way around? What if I want to _save_ a lot of data into RavenDB? That is why we have the bulk insert operation.

## Bulk inserts

Inserting data to RavenDB is pretty easy, we just have to call `Store` and then `SaveChanges`. But what happens when we want to insert a _lot_ of data? Listing 5.4 shows one such example:

```{caption="Inefficently insert new users, first option" .cs}
using (var session = documentStore.OpenSession())
{
	foreach (var userLineCsv in File.ReadLines("users.csv"))
	{
		User user = User.FromCsv(userLineCsv);
		session.Store(user);
	}
	session.SaveChanges();
}
```
Listing 5.4 is good because we'll only have to go to the database once, right? We call `SaveChanges`, and things are saved in an optimal fashion. The answer to that is a definite maybe. The problem with giving a good answer is that we are missing a very important piece of information. How big is the `users.csv` file?

If it is in the range of hundreds to low thousands of users, that is a great way to handle that. If it is more than that, we have a problem. The session isn't really meant for bulk operations. Let us assume that the `users.csv` file contains 50,000 users. That would mean that we would load 50,000 users to memory, then create a single request with a payload of half a million documents in it, then send it to the server as a single transaction.

The likely reason is that we'll get an out of memory exception at some point along the way. In particular, there is a limit to how much data can be changed in a single transaction. Admittedly, this limit is in the hundreds of megabytes^[This limit is controlled via the `Raven/Esent/MaxVerPages` option in Esent, and by the `Raven/Voron/MaxScratchBufferSize` option in  Voron.], but it is there. A very long write transaction is also something that we would like to avoid, because it is pretty costly.

A common solution is to use multiple sessions, so we'll use batch up to 512 users, then we'll call `SaveChanges`, then we create a new session. The downside of this approach is that we still have relatively large requests, and not we have a lot of them.

Trying this option with 50,000 users, we get the code in Listing 5.5. This is quick & dirty code, written merely to show a point.

```{caption="Inefficently insert new users, second option" .cs}
int amount = 0;
var session = documentStore.OpenSession();
foreach (var i in Enumerable.Range(0, 50*1000))
{
	User user = new User
	{
		Name = "Hello " +i
	};
	session.Store(user);
	if (amount++ > 512)
	{
		amount = 0;
		session.SaveChanges();
		session.Dispose();
		session = documentStore.OpenSession();
	}
}
session.SaveChanges();
session.Dispose();
```

This code runs in 21.65 seconds, or a rate of 2,310 documents/second. It is not great, but it isn't bad either. Trying with a batch size of 1024 resulted in run time of just over 25 seconds and a batch size of 256 took 26 seconds, so for this work load, 512 seems to be working fine.

Such tasks aren't frequent in RavenDB. It isn't often that you need to put so much data into the database. However, we have to deal with yet another common case. The nightly ETL process. Every night we get a file from some other system that we need to load into our system. That means that we want to be able to give you a good solution for this issue.

Just telling you to do batched `SaveChanges` isn't enough. Hence, the need for bulk insert. Bulk insert operates in the exact opposite manner than streaming. Let us see the code in Listing 5.6 and then we'll discuss what is going on.

```{caption="Efficently insert new users using bulk insert" .cs}
using (var bulkInsert = documentStore.BulkInsert())
{
	foreach (var i in Enumerable.Range(0, 50 * 1000))
	{
		User user = new User
		{
			Name = "Hello " + i
		};
		bulkInsert.Store(user);
	}
}
```

As you can see, we don't do batching in Listing 5.5. We create a single `BulkInsert` and use that. What actually happens is that the `BulkInsert` is using a single long request to talk to the server. Whenever `bulkInsert.Store` is called, we are sending that data to the server immediately. On the server side, it is also processed immediately, instead of having to wait for everything to get to the server.

> **Bulk Insert Batches**
> 
> Internally, we don't actually send each document over the network independently. We have a batch size & time limit. We batch all the documents that we get up to the batch size (whose default is 512) or until we have 200 ms without a new document being stored.
>
> At that point, we compress all the documents we have batched, and send all of them together to the server. On the server side, we read this batch, and we process all the documents in the batch as a single transaction. As a result of this, you don't have very big transactions, just a lot of small internal transactions.
> 
> It is important to understand that if the bulk insert fails midway, all the previous batches have already been committed, but all the documents in the current batch will be rolled back. In practice, this means that if the bulk insert failed, you don't know what data was committed and what wasn't.

More than anything else, the fact that we can parallelize client and server work means that we get a really good performance. It doesn't hurt that the code path that `BulkInsert` is using is also highly optimized for inserts, as you can imagine.

`BulkInsert` can also accept an options argument:

	documentStore.BulkInsert(options: new BulkInsertOptions());

Those options include:

* `BatchSize` - For how many documents should we wait before sending a batch to the server. The default is 512.
* `OverwriteExisting` - If set to true, allows RavenDB to overwrite an existing document, otherwise, an error is thrown if the inserted document already exists. Default is false, setting this to true will reduce the insert speed.
* `CheckReferencesInIndexes` - Whatever document references (resulting from `LoadDocument`) need to be checked. We'll discuss this feature in detail in the [Part II](#part-ii). Default is false, setting this to true will reduce the insert speed.
* `WriteTimeoutMilliseconds` - How much time we can wait for the full queue to clear. Default is 15,000 ms (15 seconds).

The last options deserve some additional explanation. One issue with bulk insert is that the client side can usually generate the documents far faster than the server can receive and store them. If the number of documents is high, it is possible that the number of documents waiting to be sent to the server will be very high. Eventually, you'll run out of memory.

Because of that, the `BatchSize` option also controls how many documents we can have waiting to be sent. The queued documents will be up to 150% of the size of the `BatchSize`. Assuming the `BatchSize` is set to 512, then the maximum number of pending documents will be 768. At this point, if you try to `Store` an additional document, you'll be blocked until another batch has been processed and space on the queue is freed.

To prevent a situation where you're blocked for a very long time, we use the `WriteTimeoutMilliseconds` value to make sure that a timeout exception is thrown if we are waiting for too long.

At the end of the day, inserting 50,000 documents with `BulkInsert` took 11.33 seconds, for a rate of 4,415 documents/second. Or about twice as fast as the alternative.

> **Benchmark & lies**
>
> A note about the performance numbers. I'm not trying to do a benchmark here for aboslutely the best performance. I'm actually running this with a debug build of RavenDB on both the client & server, while the system is also running integration tests in the background, on a laptop, while riding the train.  _Don't trust those numbers_!
>
> We can get to 15,000 - 20,000 sustained writes per second on standard server hardware, using a release build, and we can go beyond that by configuring the database settings properly (we'll discuss those option in Part IV - Operations). 
> 
> What I'm trying to do is to give you a _sense_ of the relative performance differences between the session and bulk insert. And in general, bulk insert in going to be between twice to ten times as fast as using `SaveChanges` and batching.

The last, but certainly not least, important thing about `BulkInsert`. If you look at Listing 5.6, you can see that it is wrapped in a `using` statement. This is _important_, the `BulkInsert` is using the `Dispose` call to flush the remaining data to the server, close the connection and in general clean after itself. It is only after the `Dispose` has completed that you can be certain that all the data that was bulk inserted is actually safe inside the database.

So far we talked about the big stuff. How we can read a lot of data and write a lot of data. Now I want to turn in the complete opposite direction and go into the very small. How can I update just a part of a document?

## Partial document updates

For the most part, when working with RavenDB you'll be working with full documents. That means that you'll load a document, modify it, and save it back to the database as a single unit. That matches closely for the idea that documents are a unit of change. 

But there are good reasons^[While there are good reasons for that, they are also pretty rare situations. For the most part, patching should be the exception] why you'll want to do just a partial document update. There are two common reasons to want to do that:

* You have a piece of data that have a good legitimate reason to be change concurrently.
* You want to save the cost of loading the full document and saving the full document.

Both reasons are somewhat problematic. Because a document is a unit of change, there should be only a single reason to change it, and all updates to a document are serialized. Reasons for wanting to change a document concurrently should be rare. One such example might be adding a comment to a blog post. Because there are no associations between comments, it is valid to have two comments being added to the same blog post at the same time.

Wanting to save the cost of loading and saving the full document is a warning sign. That usually points to a problem in the way you are structuring your documents. We'll discuss this further in the [Modeling Chapter](#modeling-ravendb-documents). For now, it is important to note that regardless of how you wish to modify a document (full update or patching), on the server side, the effect is always replacing the whole object.

With the cautionary words out of the way, the main advantages of patching are that we can handle concurrency in a more granular fashion and that we are generally sending (a lot) less data over the wire. Usually, if we have two concurrent modifications to the same document, that would generate a ConcurrencyException on one side. That is because we don't know which version should win.

With patching, we don't have the new version of the document; we have a description of the _change_ we want to make to the document. And now let us see how we can actually execute partial updates.

### Simple Patch API

RavenDB has two possible patching options. Those are called the Simple Patch and Scripted Patch. The simple patch API is fairly limited in what it can do. It has operations for:

* Set a property
* Unset (remove) a property
* Add an item to an array
* Insert an item to an array at a specified location
* Remove an item from an array at a specified location
* Increment a property
* Rename a property
* Copy a property's value to another property
* Modify a nested value by using any of the supported simple patch operations

Listing 5.7 shows an example of using the simple patch API to reduce the level of product in stock.

```{caption="Using simple patching to decrement product's in stock value" .cs}
using (var session = documentStore.OpenSession())
{
	session.Advanced.Defer(new PatchCommandData
	{
		Key = "products/1",
		Patches = new[]
		{
			new PatchRequest
			{
				Name = "UnitsInStock",
				Type = PatchCommandType.Inc,
				Value = -1
			}, 
		}
	});
	session.SaveChanges();
}
```

Additional options that you can set on the PatchRequest include what to do if the document doesn't exists (we'll run a different set of patch commands located in the `PatchIfMissing` property). Or we can specify that we'll only change a property value if its value match the `PrevVal` property, etc. 

> **`session.Advanced.Defer`**
> 
> The `Advanced.Defer` method allow you to register a low level command (such as PatchCommandData) to be carried out when the session's `SaveChanges` is called. 
> 
> This allows you to add commands to the same transaction that will occur when `SaveChanges` happen along with all the other changes in the session.

To be perfectly frank, simple patching is hard, and it isn't really nice to use or very flexible. We've added a lot of options to it over the years. But fundamentally it remained not very friendly. In general we recommend that you'll avoid using this in favor of Scripted Patching. 

### Scripted patching

RavenDB is a database for working with JSON documents. What can be more natural to work with those documents than JavaScript? And that is exactly what the Scripted Patching is providing. It allows you to write a js script and run it against a document. Listing 5.8 replicates the same behavior we have seen with simple patching in Listing 5.7.

```{caption="Using scripted patching to decrement product's in stock value" .cs}
using (var session = documentStore.OpenSession())
{
	session.Advanced.Defer(new ScriptedPatchCommandData
	{
		Key = "products/1",
		Patch = new ScriptedPatchRequest
		{
			Script = "this.UnitsInStock--;"
		}
	});
	session.SaveChanges();
}
```

There isn't much to it, is there? But at the same time, this gives your tremendous power. A more interesting script can be to split the first and last name into separate properties. We can do that using the following script (the surrounding code has been omitted for clarity's sake).

	var parts = this.Name.split();
	this.FirstName = parts[0];
	this.LastName = parts[1];
	delete this.Name;

You can also use if, while and the full power of JavaScript. In addition to the basic JavaScript library of methods, you also have available the [lodash.js](http://lodash.com/) library. Here is how we can create some random data using the lodash.js functions:

	this.Age = _.random(17, 62);

You can read more about the lodash.js method on the online documentation. For now, suffice to say that you have the ability to transform your documents, apply logic and behavior on a very granular basis. However, you should be aware that while you _can_ do a lot of stuff using patching, it is still advisable to reserve that for cases where you have no other choice.

It is usually better to work directly with the documents, and not to mess around with patching. Trying to do too much with patching means that you'll deal with a lot of scripts, and lose a lot of the nicer abilities that RavenDB gives you (client side type safety, single reason to change, etc.).

> Limitations
> 
> Because scripted patches are run on the server side, we need to be cautious about their use. RavenDB will flag and kill any script that is obviously abusive (trying to create stack overflow, infinite loop, etc.). By default, a script is limited to about 10,000 operations, after which time it is killed. 
> 
> However, especially with large documents or complex scripts, it can take a while for RavenDB to execute a script. RavenDB is doing quite a lot to optimize script execution, including caching the parsed scripts, but it is still requiring us to evaluate the scripts, and that has a non-trivial cost.

#### Parameters

Frequently, you need to customize your script to allow for different options. For example, if we look at Listing 5.8, we can see that we decrement the units in stock by one. However, what would happen if we wanted to decrement the units in stock by 7, or 5?

One *wrong* way of doing that would be the following:

	Script = "this.UnitsInStock -= " + amountToDecrement + ";";

Allow me to count the number of ways this is wrong. Just like building SQL strings using concatenations, this is wrong. It produces hard to read code, make it much harder to cache the scripts and introduces the possibility of the user input injection.

Instead, just like SQL again, we have a much better option, using parameters. See Listing 5.9.

```{caption="Using scripted patching with parameters" .cs}
using (var session = documentStore.OpenSession())
{
	session.Advanced.Defer(new ScriptedPatchCommandData
	{
		Key = "products/1",
		Patch = new ScriptedPatchRequest
		{
			Script = "this.UnitsInStock -= amountToDecrement;",
			Values = {{"amountToDecrement", 7}}
		}
	});
	session.SaveChanges();
}
```

You can see that we pass a variable `amountToDecrement` to the script. The advantage of a variable is that you don't have to worry about input injection, you don't have to build the script by string concatenation and you can fully cache the parsed script and reuse it many times.

#### Accessing other documents

One of the really nice things about the scripted patching API is that it not only give you full access to the current document (exposed as the `this` object), but it also gives you full access to _other_ documents. You can call `LoadDocument` to load another document and use its values to modify your document, as you can see in Listing 5.10. Or you can call `DeleteDocument` to remove a document, or even call `PutDocument` to create or update _another_ document. 

```{caption="Loading a related document during patching" .cs}
using (var session = documentStore.OpenSession())
{
	session.Advanced.Defer(new ScriptedPatchCommandData
	{
		Key = "products/1",
		Patch = new ScriptedPatchRequest
		{
			Script = @"
var category = LoadDocument(this.Category);
this.CategoryName = category.Name;
			",
		}
	});
	session.SaveChanges();
}
```

You can read the full details about the `DeleteDocument` and `PutDocument` methods in the online documentation, since they are far less used than `LoadDocument`.

### Concurrency

What will happen if you have to patch requests to the same document at the same time? Because they aren't full document update, it is actually possible to make this work. What will happen is that the RavenDB engine will serialize all those patch requests to the document.

Then it will execute them one at a time. Since they represent changes to the document, which is safe to do, so that one of the main use cases for patching is to concurrently update documents. That said, note that if you have a _lot_ of patch requests to the same document, eventually the internal queue RavenDB uses will be full and patch requests for this document will be rejected. If there are other operations in the same transaction, they will also fail as a single unit.

And with that, let us zoom out from the partial document updates to a far bigger scope. How to apply application wide behaviors using RavenDB using listeners?

## Listeners

It is pretty common to want to run some code whenever something happens in RavenDB. The classic example is when you want to store some audit information about who modified a document. In the previous section, we saw that we can do that manually, but that is both tedious and prone to errors or omissions. It would be much better if we could do it in a single place.

That is why the RavenDB Client API has the notion of listeners. Listeners allow you to define, in a single place, additional behavior that RavenDB will execute at particular points in time. RavenDB has the following listeners:

* `IDocumentStoreListener` - called when an entity is stored on the server.
* `IDocumentDeleteListener` - called when a document is being deleted.
* `IDocumentQueryListener` - called before a query is made to the server.
* `IDocumentConversionListener` - called when converting an entity to a document and vice versa.
* `IDocumentConflictListener` - called when a replication conflicted is encountered, this listener is discussed in depth in Chapter 10, Replication.

The store and delete listeners are pretty obvious. They are called whenever a document is stored (which can be a new document or an updated to an existing one) or when the document is deleted. A common use case for the store listener is as an audit listener, which can record which user last touched a document. A delete listener can be used to prevent deletion of a document based on your business logic, and a query listener can modify any query issued.

You can see examples of all three in Listing 5.11.

```{caption="Store, Delete and Query listeners" .cs }  
public class AuditStoreListener : IDocumentStoreListener
{
	public bool BeforeStore(string key, 
		object entityInstance, RavenJObject metadata, 
		RavenJObject original)
	{
		metadata["Last-Modified-By"] = WindowsIdentity
			.GetCurrent().Name;
		return false;
	}

	public void AfterStore(string key, 
		object entityInstance, RavenJObject metadata)
	{
	}
}

public class PreventActiveUserDeleteListener : 
	IDocumentDeleteListener
{
	public void BeforeDelete(string key, 
		object entityInstance, RavenJObject metadata)
	{
		var user = entityInstance as User;
		if (user == null)
			return;
		if (user.IsActive)
			throw new InvalidOperationException(
				"Cannot delete active user: " +
				 user.Name);
	}
}

public class OnlyActiveUsersQueryListener : 
	IDocumentQueryListener
{
	public void BeforeQueryExecuted(
		IDocumentQueryCustomization queryCustomization)
	{
		var userQuery = queryCustomization as
			IDocumentQuery<User>;
		if (userQuery == null)
			return;
		userQuery.AndAlso().WhereEquals("IsActive", true);
	}
}
```
In the `AuditStoreListener`, we modify the metadata to include the current user name. Note that we return `false` from the `BeforeStore` method as an indication that we didn't change the `entityInstance` parameter. This is an optimization step, so we won't be forced to re-serialize the `entityInstance` if it wasn't changed by the listener.

In the `PreventActiveUserDeleteListener` case, we throw if an active user is being deleted. This is very straightforward and easy to follow. It is the case of `OnlyActiveUsersQueryListener` that is interesting. Here we check if we are querying on users (by checking if the query to customize is an instance of `IDocumentQuery<User>`) and if it is, we also add a filter on active users only. In this manner, we can ensure that all user queries will operate only on active users.

We register the listeners on the document store during the initialization. Listing 5.12 shows the updated `CreateDocumentStore` method on the `DocumentStoreHolder` class.

```{caption="Registering listeners in the document store" .cs }   
private static IDocumentStore CreateDocumentStore()
{
	var documentStore = new DocumentStore
	{
		Url = "http://localhost:8080",
		DefaultDatabase = "Northwind",
	};

	documentStore.RegisterListener(
			new AuditStoreListener());
	documentStore.RegisterListener(
			new PreventActiveUserDeleteListener());
	documentStore.RegisterListener(
			new OnlyQueryActiveUsers());

	documentStore.Initialize();
	return documentStore;
}
```

Once registered, the listeners are active and will be called whenever their respected actions occur. 

The `IDocumentConversionListener` allows you a fine grained control over the process of the conversion process of entities to documents and vice versa. If you need to pull data from an additional system when a document is loaded, this is usually the place where you'll put it^[That said, pulling data from secondary sources on document load is frowned upon, documents are coherent and independent. You shouldn't require additional data, and that is usually a performance problem].

A far more common scenario for conversion listener is to handle versioning, whereby you modify the old version of the document to match an update entity definition on the fly. This is a way for you to do rolling migrations, without an expensive stop-the-world step along the way.

While the document conversion listener is a great aid in controlling the conversion process, if all you care about is the actual serialization, without the need to run your own logic, it is probably best to go directly to the serializer and use that.

## The Serialization Process

RavenDB uses the [Newtonsoft.JSON](http://james.newtonking.com/json) library for serialization. This is a very rich library with quite a lot of options and levers that you can tweak. 
Because of version incompatibilities between RavenDB and other libraries that also has a dependeny on Newtonsoft.JSON, RavenDB has internalized the Newtonsoft.JSON library. 
To access the RavenDB copy of Newtonsoft.JSON, you need to use the following namespace: `Raven.Imports.Newtonsoft.Json`. 

Newtonsoft.JSON has several options for customizing the serialization process. One of those is a set of attributes (`JsonObjectAttribute`, `JsonPropertyAttribute`, etc). Because RavenDB has its own copy, it is possible to have two sets of such attributes: one for serialization of the entity to a document in RavenDB, and another for serialization of the document for external consumption.

Another method of customizing the serialization in Newtonsoft.JSON is using the `documentStore.Conventions.CustomizeJsonSerializer` event.
Whenever a serializer is created by RavenDB, this event is called and allows you to define the serializer's settings. You can see an example of that in Listing 5.13.

```{caption="Customizing the serialization of money" .cs }   
DocumentStoreHolder.Store.Conventions
	.CustomizeJsonSerializer += serializer => 
	{
		serializer.Converters.Add(new JsonMoneyConverter());
	};

public class JsonMoneyConverter : JsonConverter
{
	public override void WriteJson(JsonWriter writer, 
		object value, JsonSerializer serializer)
	{
		var money = (Money) value;
		writer.WriteValue(money.Amount + " " + money.Currency);
	}

	public override object ReadJson(JsonReader reader, 
		Type objectType, object existingValue, 
		JsonSerializer serializer)
	{
		var parts = reader.ReadAsString().Split();
		return new Money
		{
			Amount = decimal.Parse(parts[0]),
			Currency = parts[1]
		};
	}

	public override bool CanConvert(Type objectType)
	{
		return objectType == typeof (Money);
	}
}

public class Money
{
	public string Currency { get; set; }
	public decimal Amount { get; set; }
}
```
The idea in Listing 5.13 is to have a `Money` object that holds both the amount and the currency, but to serialize it to JSON as a string property. So a `Money` object representing 10 US Dollars would be serialized to the following string: "10 USD".

The JsonMoneyConverter converts to and from the string representation and the json serializer customization event registers the converter with the serializer. Note that this is probably not a good idea, and you will want to store the `Money` without modifications, so you can do things like sum up order by currency, or actually work with the data.

I would only consider using this approach as an intermediary step, probably as part of a migration if I had two versions of the application working concurrently on the same database.

## Changes() API

Assume that we have a user busy working on a document. And that process can take a while. In the meanwhile, another user came in and changes that document. What would be the experience from the point of the user? Well, either we let the Last Write Win, or we use Optimistic Concurrency. Either way we're going to have to annoy someone. How about being able to notify the user, as soon as the document has updated, that it needs to be refreshed?
But that would require us to check with the server periodically to check if the document has changed, and that isn't a nice solution to have.

Polling is wasteful, most of the time you spend a lot of time asking the same question and expecting to get the same answer. Anyone who had to deal with "are we there yet?" and "are we there yet, _now_?" knows how annoying that can be. Setting aside that, we have to consider load and latency factors as well. In short, we don't want to do polling. So it is good that we don't have to. Listing 5.14 shows us how, note that this code uses the Reactive Extensions package (`Install-Package Rx-Core` using NuGet).


```{caption="Registering for Changes() notifications" .cs }   
var stopSubscription = documentStore.Changes()
	.ForDocument("products/2")
	.Subscribe(notification =>
	{
		string msg = notification.Type + " on document " + notification.Id;
		Console.WriteLine(msg);
	});

//change products/2
Console.ReadLine(); 

// stop getting notifications for products/2
stopSubscription.Dispose();

```

Using the code in Listing 5.14, we registered for all changes (Put, Delete) for the `products/2` document. We can take actions, such as notify the user (using SignalR if we are running in web application, for example). You can register for notifications on specific documents, all documents with a specific prefix or of a specific collection, for all documents changes or for updates to indexes.

Notice that the change notification include the document (or index) id and the type of the operation performed. Put or Delete in the case of documents, most often. If you want to actually access the document in question, you'll need to load it using a session.

Another important issue when dealing with notifications: once subscribed, you'll continue to get notifications for the subscriptions until you have disposed the subscription (the last line in Listing 5.14). And obviously, your subscription is going to be called on another thread, so you need to be aware that you might need to marshal your action to the appropriate location.

And this is pretty much it for Changes(), it is a powerful feature, and it enable a whole host of interesting scenarios. But from external point of view, it is also drop dead simple to work with and use. And with that in mind, let us look at another such feature, result transformers and what you can do with them.

## Result transformers

Result transformers are server side transformations that allow us to project specific data to the client. That is a very impressive statement, but what does this _mean_?

Let us take a simple example: we want to show a few details about an order, such as the order's date, the company name and its id.

We can do that using the following code:

	var order = session.Include<Order>(x => x.Company)
		.Load(orderId);
	var company = session.Load<Company>(order.Company);

	Console.WriteLine("{0}\t{1}\t{2}", order.Id, company.Name, order.OrderedAt);

This is pretty simple, and because we used an `Include`, we only have to go to the server once. But note that we have to load the entire `Order` and `Company` documents to run this code, even though we only want a few properties.  We had to send over a kilobyte of data over the wire, while we actually show the user a lot less data.

> **Creating transformers on the server**
> 
> Because result transformers are server side artifacts, you need to create them on the server before they can be used. You can do that manually at application startup using the following code (to create the transformer shown in Listing 5.15):
> 
> 	new JustOrderIdAndcompany().Execute(documentStore);
>
> Or (as you'll see in the Chapter 6 - Indexing) using the `IndexCreation.CreateIndexes` to create all indexes and transformers, which is the more commonly used method.
> It is safe to call either method when the transformer already exists, in those cases, if the transformer has changed it will be updated, if the transformer hasn't changed, this will result in a no-op.

The Northwind dataset is pretty small, and this is somewhat of an extreme example, but the principle holds. We don't have infinite bandwidth, after all^[Remember the fallacies of distributed computing!]. So we don't want to pay this price. How can we get just the information that we want? Listing 5.15 shows a simple result transformer.

```{caption="Simple result transformer and its usage" .cs }   
public class JustOrderIdAndcompany : AbstractTransformerCreationTask<Order> //todo: this line of code is cut off in the released pdf
{
	public class Result
	{
		public string Id { get; set; }
		public string Company { get; set; }
		public DateTime OrderedAt { get; set; }
	}

	public JustOrderIdAndcompany()
	{
		TransformResults = orders =>
			from order in orders
			select new { order.Id, order.Company, order.OrderedAt };
	}
}

// usage

var order = session.Load<JustOrderIdAndcompany, JustOrderIdAndcompany.Result>(orderId);//todo: this line of code is cut off in the released pdf
var company = session.Load<Company>(order.Company);

Console.WriteLine("{0}\t{1}\t{2}", order.Id, company.Name, order.OrderedAt);//todo: this line of code is cut off in the released pdf

```

Using the `JustOrderIdAndcompany` transformer, we were able to load just those specific properties from the document, saving the cost of loading the entire document. However, we are now no longer loading the associated company document, so we have to do two calls to the server to get all the information we need. Having to go to the server twice is a bummer, but there is a reason we don't include the company document. We have a far better option available for us, `LoadDocument`, which we'll discuss in the next section. 

### Load Document

Result transformers are pretty cool, being able to select which properties to project can save a lot in bandwidth. But what makes them a truly awesome feature is the ability to reference other documents. Listing 5.16 shows a much better version of the transformer.

```{caption="Projecting data from multiple documents in a result transformer" .cs }   
public class JustOrderIdAndcompanyName : AbstractTransformerCreationTask<Order>
{
	public class Result
	{
		public string Id { get; set; }
		public string CompanyName { get; set; }
		public DateTime OrderedAt { get; set; }
	}

	public JustOrderIdAndcompanyName()
	{
		TransformResults = orders =>
			from order in orders
			let company = LoadDocument<Company>(order.Company)
			select new { order.Id, CompanyName = company.Name, order.OrderedAt };
	}
}


// usage
var order = session
  .Load<JustOrderIdAndcompanyName,JustOrderIdAndcompanyName.Result>(orderId);

Console.WriteLine("{0}\t{1}\t{2}", order.Id,
	order.CompanyName, order.OrderedAt);

```

Notice what we are doing in the `JustOrderIdAndcompanyName` transformer. We are using the `LoadDocument` method inside the transformer to load the associated company document, then we project just the company name out to the client.

This feature gives us complete control over how to shape the data from the server. And being able to pull data from associated documents is very powerful. Just to complete this discussion, the over the wire cost is less than 300 bytes. And in real world situations, the actual saving is far more impressive. 

You aren't limited to just one document to be loaded. We could have gotten the employee's name for this order, in much the same way we go the company's name.

This is actually just a taste of what you can do with transformers. We'll run into them again when we discuss querying indexes, which is where result transformers really shine.

## Summary

We covered a lot of ground in this chapter. We started by talking about lazy loading and how we can use it to reduce the number of remote calls we are making, then diverged into talking about Safe by Default and how we have a budget that limits the number of remote calls we can make. Hence, the numerous ways that we have in the RavenDB Client API to reduce the number of remote calls you make (and along the way, improve your application's perfromance). 

Next, and along the same lines, we covered another Safe by Default topic, preventing Unbounded Result Sets. RavenDB does that by specifying a limit (of 128 results) to the number of results a query will return, if you don't specify such a limit yourself, and by enforcing a maximum upper limit (of 1,024 results, by default) for the total number of results that can be specified. 

But whenever there is a limit, there is also a way to avoid it. And no, that isn't by using `.Take(int.MaxValue)`. You can get the entire result set, regardless of size, when you using the Streaming API. This API is meant to deal with large number of results, and it will stream results on both client and server so you can parallelize the work.

The other side of getting a very large result set from the database is actually inserting a lot of data to the database. And we looked at doing that using `SaveChanges` and then using the dedicated API for that, `BulkInsert`. From the very big, we moved to the very small, looking at how we can do partial document updates using patching. We looked at simple patching and scripted patching, what we can do with them and how they work.

And from updating just part of a document, we moved to handling cross cutting concerns via listeners. More specifically, how you can use listeners to provide application wide behavior from a single location. For example, handling auditing metadata in a listener means that you never have to worry about forgetting to write the audit code. We also learned about the serialization process and how you can have fine grained control over everything that goes on during serialization and deserialization.

Closing this chapter, we look at the Changes() API, which allows us to register for notifications from RavenDB whenever a document changes (as well as a host of other stuff) and result transformers, which allow us to run a server side transformation of the data before it is sent to the client.

Wow, that was a lot of stuff to go through. This concludes Part I, which gives you the basic tools of how to use RavenDB. Next, we are going to start talking about indexing, and all the exciting things that you can do with them. Go get some coffee: you're going to want to be awake for what is coming.
